# -*- coding: utf-8 -*-
"""TopicosI_Trabalho.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NPV7uBj-kJ2e3VbpgVsO21ok8gSNemJl
"""

### ---------- Imports ----------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

#### decision tree
from sklearn import tree
from sklearn.model_selection import cross_val_score, cross_val_predict

#### random forest
from sklearn.ensemble import RandomForestClassifier

"""# Read database without preprocessing"""

### ---------- Read database ----------
dataset = pd.read_csv('diabetes.csv', sep=';')

dataset.head()

### ---------- verificando o tamanho do dataset ----------
print("Informações Sobre o Dataset")
print("Variáveis:\t {}".format(dataset.shape[1]))
print("Entradas:\t {}\n".format(dataset.shape[0]))
print("O dataset apresenta dados ausentes:\n")

### ---------- identificando os tipos das variáveis ----------
display(dataset.info())

### ----------- distribuição estátistica da base de dados -----------
dataset.describe()

### ----------- distribuição estatística das variáveis categóricas -----------
dataset.describe(include='O')

### ----------- Função para retornar o percentual de dados ausentes do dataframe -----------
def missing_values(df):
        mis_val = df.isnull().sum()
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        print ("O DataFrame selecionado tem " + str(df.shape[1]) + " colunas.\n"      
            "Há " + str(mis_val_table_ren_columns.shape[0]) +
              " colunas com valores ausentes")
        return mis_val_table_ren_columns
missing_values(dataset)

"""# Separate columns with relevant data"""

### ---------- Create new data set with selected questions ----------
newDataset = dataset[['V0001',
                      'C006', 'C00701', 'C00702', 'C00703','C008', 
                      'I001', 
                      'P001', 'P002', 'P008', 'P009', 'P016', 'P017', 'P018', 'P019', 'P020', 'P021', 'P022', 'P025',
                      'P026', 'P027', 'P028', 'P029', 'P031', 'P034', 'P035', 'P036', 'P03701', 'P03702', 'P038', 'P039',
                      'P040', 'P04101', 'P04102', 'P045', 'P046', 'P047', 'P048', 'P049',
                      'Q029', 'Q030', 'Q031', 'Q032', 'Q033', 'Q035', 'Q036', 'Q037', 'Q038', 'Q039', 'Q040', 'Q041', 
                      'Q042', 'Q043', 'Q04701', 'Q04702', 'Q04703', 'Q04704', 'Q04705', 'Q048', 'Q050', 'Q05501', 'Q05502', 
                      'Q05503', 'Q05504', 'Q05505', 'Q05506', 'Q05507', 'Q05508', 'Q05509', 'Q056', 'Q057', 'Q058' ]]
newDataset.head()

### ---------- verificando o tamanho do newDataset ----------
print("Informações Sobre o Dataset")
print("Variáveis:\t {}".format(newDataset.shape[1]))
print("Entradas:\t {}\n".format(newDataset.shape[0]))
print("O dataset apresenta dados ausentes:\n")

### ---------- identificando os tipos das variáveis ----------
display(newDataset.info())

### ----------- distribuição estátistica da base de dados -----------
newDataset.describe()

### ----------- distribuição estatística das variáveis categóricas -----------
newDataset.describe(include='O')

missing_values(newDataset)

"""# Delete columns with more than 40% of missing data"""

### ---------- Columns deleted: Q057,  Q033, P049, Q037*, P048, P036*, Q036, Q035, Q038, Q048, Q04702, Q05503, Q05501, Q050, Q04705, Q04704, Q04703 ----------
### ---------- Columns deleted: Q05504, Q04701, Q043, Q042, Q041, Q040, Q05502, Q056, Q05509, Q05508, Q05507, Q05506, Q05505, P038, P039, P040 ----------
### ---------- Columns deleted:  Q031, Q032, Q039, Q058, P021, P022 ----------
### ---------- Columns deleted:  P008, P017, P019 ----------
### ---------- Total of delete columns: 42 ----------
datasetFillOfData = newDataset[['V0001',
                                'C006', 'C00701', 'C00702', 'C00703','C008', 
                                'I001', 
                                'P001', 'P002', 'P009', 'P016', 'P018', 'P020', 'P025', 'P026', 'P027', 
                                'P028', 'P029', 'P031', 'P034', 'P035', 'P03701', 'P03702', 'P04101', 'P04102', 'P045', 'P046', 'P047',
                                'Q029', 'Q030']]
datasetFillOfData.head()

### ---------- verificando o tamanho do datasetFillOfData ----------
print("Informações Sobre o Dataset")
print("Variáveis:\t {}".format(datasetFillOfData.shape[1]))
print("Entradas:\t {}\n".format(datasetFillOfData.shape[0]))
print("O dataset apresenta dados ausentes:\n")

### ---------- identificando os tipos das variáveis ----------
display(datasetFillOfData.info())

### ----------- distribuição estátistica da base de dados -----------
datasetFillOfData.describe()

### ----------- distribuição estatística das variáveis categóricas -----------
datasetFillOfData.describe(include='O')

missing_values(datasetFillOfData)

"""# Using moda to fill columns with missing data"""

### ----------- Fill dataset with moda -----------
#pd.options.mode.chained_assignment = None  

#datasetFillOfData['P017'] = datasetFillOfData['P017'].fillna(datasetFillOfData['P017'].mode()[0])
#datasetFillOfData['P008'] = datasetFillOfData['P008'].fillna(datasetFillOfData['P008'].mode()[0])
#datasetFillOfData['P019'] = datasetFillOfData['P019'].fillna(datasetFillOfData['P019'].mode()[0])
#missing_values(datasetFillOfData)

"""# Graphs after removing columns"""

plt.rcParams["figure.figsize"] = (10,5)
datasetFillOfData.groupby(['V0001','Q030']).size().groupby(level=0).apply(
     lambda x: 100 * x / x.sum()
 ).to_frame().unstack().plot(kind='bar',stacked=False,legend=False)
plt.title('Diabetes')
plt.xlabel('Estado')
plt.legend(labels=['Recebeu diagnóstico', 'Não recebeu diagnóstico'])
plt.show()

datasetFillOfData.groupby(['C008', 'Q030']).size().groupby(level=0).apply(
     lambda x: 100 * x / x.sum()
 ).to_frame().unstack().plot(kind='line',stacked=False,legend=True)
plt.title('Diabetes por idade')
plt.xlabel('Idade')
plt.legend(labels=['Recebeu diagnóstico', 'Não recebeu diagnóstico'])
plt.show()

"""# Removing rows in dataset which is outside of track ages 13 at 40"""

### ----------- Deletando linhas que estão fora da faixa de idade entre 13 anos até 40 anos -----------
trueDataset = datasetFillOfData
#trueDataset.shape

trueDataset = trueDataset.loc[(trueDataset['C008'] > 13) & (trueDataset['C008'] < 40)]
trueDataset.shape

trueDataset.head()

### ---------- verificando o tamanho do trueDataset ----------
print("Informações Sobre o Dataset")
print("Variáveis:\t {}".format(trueDataset.shape[1]))
print("Entradas:\t {}\n".format(trueDataset.shape[0]))
print("O dataset apresenta dados ausentes:\n")

### ---------- identificando os tipos das variáveis ----------
display(trueDataset.info())

### ----------- distribuição estátistica da base de dados -----------
trueDataset.describe()

### ----------- distribuição estatística das variáveis categóricas -----------
trueDataset.describe(include='O')

"""# Graphs after removing rows"""

plt.rcParams["figure.figsize"] = (10,5)
trueDataset.groupby(['V0001','Q030']).size().groupby(level=0).apply(
     lambda x: 100 * x / x.sum()
 ).to_frame().unstack().plot(kind='bar',stacked=False,legend=False)
plt.title('Diabetes')
plt.xlabel('Estado')
plt.legend(labels=['Recebeu diagnóstico', 'Não recebeu diagnóstico'])
plt.show()

trueDataset.groupby(['C008', 'Q030']).size().groupby(level=0).apply(
     lambda x: 100 * x / x.sum()
 ).to_frame().unstack().plot(kind='line',stacked=False,legend=True)
plt.title('Diabetes por idade')
plt.xlabel('Idade')
plt.legend(labels=['Recebeu diagnóstico', 'Não recebeu diagnóstico'])
plt.show()

"""# Implements Decision Tree"""

### ---------- Set criteria for Decision Tree ----------
decisionTree = tree.DecisionTreeClassifier(criterion="entropy")

### ---------- Features of dataset ----------
### objects: P028, P029, P031, P035, P03701, P03702, P04101 e P04102
diabetesData = trueDataset[['V0001',
                            'C006', 'C00701', 'C00702', 'C00703','C008', 
                            'I001', 
                            'P001', 'P002', 'P009', 'P016', 'P018', 'P020', 'P025', 'P026', 'P027', 
                            #'P028', 'P029', 'P031', 'P034', 'P035', 'P03701', 'P03702', 'P04101', 'P04102', 
                            'P045', 'P046', 'P047',
                            'Q029']]
diabetesTarget = trueDataset[['Q030']]

### ---------- Define Tree ------------
diabetesTree = decisionTree.fit(diabetesData, diabetesTarget)

### ---------- Predict ----------
diabetesTree.predict(diabetesData)

### ---------- Apply cross-validation ----------
allScores = cross_val_score(diabetesTree, diabetesData, diabetesTarget, scoring='accuracy', cv=5)

allScores.mean()

### ---------- Apply cross-validation ----------
allPredict = cross_val_predict(diabetesTree, diabetesData, diabetesTarget, cv=5)

allPredict.mean()

"""# Implements Random Forest"""

### ---------- Set criteria for Random Forest ----------
randomForest = RandomForestClassifier(criterion="entropy")

### ---------- Define forest ----------
diabetesForest = randomForest.fit(diabetesData, diabetesTarget)

### ---------- Predict ----------
diabetesForest.predict(diabetesData)

### ---------- Apply validation ----------
allScoresForest = cross_val_score(diabetesForest, diabetesData, diabetesTarget, scoring='accuracy', cv=5)

print(allScoresForest.mean())

### ---------- Apply validation ----------
allPredictForest = cross_val_score(diabetesForest, diabetesData, diabetesTarget, cv=10)

print(allPredictForest.mean())